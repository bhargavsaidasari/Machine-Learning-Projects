{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English-Hindi NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU2e7f/ZH2gP150zZU3PVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargavsaidasari/Machine-Learning-Projects/blob/master/English_Hindi_NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9si8kPTAbWNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VFCcEAzhuwD",
        "colab_type": "text"
      },
      "source": [
        "## Load the Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy_Jh2d8hcEJ",
        "colab_type": "code",
        "outputId": "2f6ceb7f-53fa-414c-d714-7e9183a1f351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "dataset = pd.read_csv('/content/Hindi_English_Truncated_Corpus.csv', encoding='utf-8')\n",
        "dataset.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ted</td>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ted</td>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ted</td>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      source  ...                                     hindi_sentence\n",
              "0        ted  ...  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...\n",
              "1        ted  ...  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...\n",
              "2  indic2012  ...   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।\n",
              "3        ted  ...     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते\n",
              "4  indic2012  ...        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBOr3mnwmfAG",
        "colab_type": "code",
        "outputId": "358aad4c-f509-4c81-9c70-ef27feb96d75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "dataset2 = pd.read_csv('english_to_hindi.txt', error_bad_lines=False, \n",
        "                       sep='\\t', header=None, encoding='utf-8')\n",
        "dataset2.columns = ['english_sentence', 'hindi_sentence']\n",
        "dataset2.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Help!</td>\n",
              "      <td>बचाओ!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>उछलो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>कूदो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>छलांग.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>नमस्ते।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  english_sentence hindi_sentence\n",
              "0            Help!          बचाओ!\n",
              "1            Jump.          उछलो.\n",
              "2            Jump.          कूदो.\n",
              "3            Jump.         छलांग.\n",
              "4           Hello!        नमस्ते।"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqzc0PKss9Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.concat([dataset, dataset2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK2kGUndlWKw",
        "colab_type": "text"
      },
      "source": [
        "## Sources of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu4a5S5Ah2A5",
        "colab_type": "code",
        "outputId": "5fce904e-4773-4f30-e027-8faee65aed85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset['source'].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ted', 'indic2012', 'tides', nan], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjWNTx5BlaKw",
        "colab_type": "text"
      },
      "source": [
        "## Length of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-um8lV8fQGv",
        "colab_type": "code",
        "outputId": "7f896b37-4bd6-4918-d0ec-28a40f2016ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(dataset.shape[0])\n",
        "# Randomly select 50000 sentences to train on \n",
        "\n",
        "dataset = dataset.sample(n=70000, random_state=42)\n",
        "dataset = dataset.sample(frac=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7nJ0KHilyPq",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing steps\n",
        "### 1) Remove NaN rows\n",
        "### 2) Lower Case\n",
        "### 3) Remove punctuation\n",
        "### 4) Add start and end tags for target sequences\n",
        "### 5) One hot encoding for both the target and train sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynRaQpnmykef",
        "colab_type": "text"
      },
      "source": [
        "#### Remove NaN Rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYROneAzyZvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.dropna(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D_jnIsAt6GN",
        "colab_type": "text"
      },
      "source": [
        "#### Lower case\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOFYgSQkt72c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['english_sentence'] = dataset['english_sentence'].apply(lambda x: x.lower())\n",
        "dataset['hindi_sentence'] = dataset['hindi_sentence'].apply(lambda x: x.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmQhrx-suvPN",
        "colab_type": "text"
      },
      "source": [
        "#### Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NqMdCHqt0IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "  stripped = [w.translate(table) for w in sentence.split()]\n",
        "  sentence = ' '.join(stripped) \n",
        "  return(sentence)\n",
        "\n",
        "\n",
        "dataset['english_sentence'] = dataset['english_sentence'].apply(lambda x: remove_punctuation(x))\n",
        "dataset['hindi_sentence'] = dataset['hindi_sentence'].apply(lambda x: remove_punctuation(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6lDiosy7QU",
        "colab_type": "text"
      },
      "source": [
        "#### Add start and end tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGbU8zXay9-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['hindi_sentence'] = dataset['hindi_sentence'].apply(lambda x: '<start> ' + x + ' <end>')\n",
        "dataset['english_sentence'] = dataset['english_sentence'].apply(lambda x: x + ' <end>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK8hYEtrkT64",
        "colab_type": "text"
      },
      "source": [
        "#### Find maximum length of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmUkICPykWef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset['length_english'] = dataset['english_sentence'].apply(lambda x: len(x.split()))\n",
        "dataset['length_hindi'] = dataset['hindi_sentence'].apply(lambda x: len(x.split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqdczqfMmMqF",
        "colab_type": "code",
        "outputId": "6fea2c21-225f-4a6c-819d-58b0ae2dce76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset = dataset.loc[dataset['length_english'] < 20]\n",
        "dataset = dataset.loc[dataset['length_hindi'] < 20]\n",
        "print(dataset.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDdUGvhygFOP",
        "colab_type": "text"
      },
      "source": [
        "#### One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZWqvix5jIkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "def create_lookup_tables(text):\n",
        "    \"\"\"\n",
        "    Create lookup tables for vocabulary.\n",
        "\n",
        "    params: \n",
        "      text: The text of tv scripts split into words\n",
        "\n",
        "    returns: \n",
        "      A tuple of dicts (vocab_to_int, int_to_vocab)\n",
        "    \"\"\"\n",
        "    words_frequency = Counter(text)\n",
        "    sorted_frequency = sorted(words_frequency, key=words_frequency.get, reverse=True)\n",
        "    int_to_vocab = {ii+1: ch for ii, ch in enumerate(sorted_frequency)}\n",
        "    vocab_to_int = {ch: ii for ii, ch in int_to_vocab.items()}\n",
        "    # return tuple\n",
        "    return (vocab_to_int, int_to_vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJaqJ5VsgJ_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_english_words = []\n",
        "all_hindi_words = []\n",
        "for i in range(dataset.shape[0]):\n",
        "  all_english_words.extend(dataset['english_sentence'].iloc[i].split())\n",
        "  all_hindi_words.extend(dataset['hindi_sentence'].iloc[i].split())\n",
        "\n",
        "english_vocab_to_int, english_int_to_vocab = create_lookup_tables(all_english_words)\n",
        "hindi_vocab_to_int, hindi_int_to_vocab = create_lookup_tables(all_hindi_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu0hWM6zt0D0",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vviHkbbat2kR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_int_text = []\n",
        "hindi_int_text = []\n",
        "for i in range(dataset.shape[0]):\n",
        "  english_int_text.append([english_vocab_to_int[word] for word in dataset['english_sentence'].iloc[i].split()])\n",
        "  hindi_int_text.append([hindi_vocab_to_int[word] for word in dataset['hindi_sentence'].iloc[i].split()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiqH8n8gnQnL",
        "colab_type": "text"
      },
      "source": [
        "#### Append zeros to make each row in the data equal length for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsNvBfHxnPpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_sequence(sentence_list):\n",
        "  return(max(len(x) for x in sentence_list))\n",
        "\n",
        "def padded_sequence(sentence_list):\n",
        "  max_length = max_sequence(sentence_list)\n",
        "  for i in range(len(sentence_list)):\n",
        "    sentence_list[i].extend([0] * (max_length - len(sentence_list[i])))\n",
        "  return(np.array(sentence_list)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oNvR9l51DA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_int_text = padded_sequence(english_int_text)\n",
        "hindi_int_text = padded_sequence(hindi_int_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNZP9ode2I1-",
        "colab_type": "text"
      },
      "source": [
        "#### Train-Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlFzrW7q2IE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_length = int(0.8 * english_int_text.shape[0])\n",
        "# English train and test sets\n",
        "english_train_text = english_int_text[:train_length]\n",
        "english_test_text = english_int_text[train_length:]\n",
        "# Hindi train and test sets\n",
        "hindi_train_text = hindi_int_text[:train_length]\n",
        "hindi_test_text = hindi_int_text[train_length:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYgiEvGTvN00",
        "colab_type": "text"
      },
      "source": [
        "#### DataLoader Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YH6dUKcvQJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def batch_data(train_english, test_hindi, batch_size):\n",
        "  \"\"\"\n",
        "    Batch the neural network data using DataLoader.\n",
        "\n",
        "    Params: \n",
        "      words: The word ids of the TV scripts\n",
        "      sequence_length: The sequence length of each batch\n",
        "      batch_size: The size of each batch; the number of sequences in a batch\n",
        "\n",
        "    Returns: \n",
        "      DataLoader with batched data\n",
        "  \"\"\"\n",
        "  train_data = TensorDataset(torch.from_numpy(train_english), torch.from_numpy(test_hindi))\n",
        "  loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "  return(loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9QbAnFvjNwI",
        "colab_type": "code",
        "outputId": "27e4cfc7-ec62-4c80-9a56-028a2baf8ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataloader = batch_data(english_train_text, hindi_train_text, 64)\n",
        "# Test the dataloader\n",
        "for item in iter(dataloader):\n",
        "  train, test = item\n",
        "  print(\"Training Size is: \" + str(train.shape))\n",
        "  print(\"Testing Size is: \" + str(test.shape))\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Size is: torch.Size([64, 19])\n",
            "Testing Size is: torch.Size([64, 19])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxCAEyRZmAvS",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXzZ-uT0mDcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderGRU(nn.Module):\n",
        "  def __init__(self, vocab_size, embedded_size, encoding_size, max_len=9):\n",
        "    super(EncoderGRU, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedded_size\n",
        "    self.encoding_size = encoding_size\n",
        "    self.seq_len = max_len\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "    self.gru = nn.GRU(input_size=self.embedding_size, hidden_size=self.encoding_size,\n",
        "                      bidirectional=True, num_layers=2)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input).permute(1, 0, 2)\n",
        "    output = embedded\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self, batch_size):\n",
        "    return torch.zeros(2*2, batch_size, self.encoding_size, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcVGUaw2Eya4",
        "colab_type": "text"
      },
      "source": [
        "#### Test Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z48D53yvk5mA",
        "colab_type": "code",
        "outputId": "bdaf3b43-d8cc-4844-806a-f4e2804534a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = EncoderGRU(len(english_int_to_vocab), 256, 256, 20)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "english, hindi = next(iter(dataloader))\n",
        "\n",
        "hidden = encoder.initHidden(64)\n",
        "enc_output, enc_hidden = encoder(english.to(device), hidden)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([19, 64, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10AZVjEaE1Hh",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf_XJjc7E3Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdannauAttention(nn.Module):\n",
        "  def __init__(self, hindi_vocab_size, embedding_size, attention_hidden_size, decoding_size,\\\n",
        "               encoding_size, seq_len=9):\n",
        "    super(BahdannauAttention, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.vocab_size = hindi_vocab_size\n",
        "    self.decoding_size = decoding_size\n",
        "    self.encoding_size = encoding_size\n",
        "    self.hidden_size = attention_hidden_size\n",
        "    self.seq_len = seq_len\n",
        "    # inputs to output\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
        "    self.gru = nn.GRU(input_size=self.embedding_size + self.encoding_size, \n",
        "                      hidden_size=self.decoding_size, num_layers=2)\n",
        "    self.fc = nn.Linear(self.decoding_size, self.vocab_size)\n",
        "    # attention network\n",
        "    self.attention_input_encoder = nn.Linear(self.encoding_size, self.hidden_size)\n",
        "    self.attention_prev_state = nn.Linear(self.decoding_size, self.hidden_size)\n",
        "    self.v = nn.Linear(self.hidden_size, 1)\n",
        "\n",
        "\n",
        "  def forward(self, input, hidden, prev_encode):\n",
        "    prev_encode = prev_encode.permute(1, 0, 2)\n",
        "    hidden_permute = hidden.permute(1, 0, 2)\n",
        "    score = torch.tanh(self.attention_input_encoder(prev_encode) + \\\n",
        "                       torch.mean(self.attention_prev_state(hidden_permute), dim=1).unsqueeze(1))\n",
        "    attention_weights = F.softmax(self.v(score), dim=1)\n",
        "    context_vector = attention_weights * prev_encode\n",
        "    context_vector = torch.sum(context_vector, dim=1)\n",
        "    # compute embeddings of input\n",
        "    embeddings = self.embedding(input)\n",
        "    # concatanate embeddings and context vector\n",
        "    x = torch.cat((context_vector.unsqueeze(1), embeddings), -1).permute(1, 0, 2)\n",
        "    output, hidden = self.gru(x, hidden)\n",
        "    output = output.view(-1, self.decoding_size)\n",
        "    final_output = self.fc(output)\n",
        "    return final_output, hidden, attention_weights\n",
        "\n",
        "  def initHidden(self, batch_size):\n",
        "    return torch.zeros(2, batch_size, self.decoding_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9_SkvHUE4il",
        "colab_type": "text"
      },
      "source": [
        "#### Test Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9ldV7n3E7LH",
        "colab_type": "code",
        "outputId": "fdc16153-c690-43db-8f4a-013ca6d01c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "decoder = BahdannauAttention(len(hindi_int_to_vocab), 256, 256, 512, 256 * 2, 9)\n",
        "\n",
        "decoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "english, hindi = next(iter(dataloader))\n",
        "\n",
        "hidden = decoder.initHidden(64)\n",
        "decoder_output, decoder_hidden, _ = decoder(hindi[:,0].unsqueeze(1).to(device), hidden, enc_output)\n",
        "\n",
        "print(decoder_output.size()) # max_length, batch_size, enc_units\n",
        "print(decoder_hidden.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 28693])\n",
            "torch.Size([2, 64, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Prx6LIjIIY",
        "colab_type": "text"
      },
      "source": [
        "#### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqeaAVzLX3cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = EncoderGRU(len(english_int_to_vocab) + 1, 256, 256, 20)\n",
        "decoder = BahdannauAttention(len(hindi_int_to_vocab) + 1, 256, 256, 256*2, 256*2, 20)\n",
        "\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgCsYhJ0YFM9",
        "colab_type": "code",
        "outputId": "3a1043ad-b9d1-4d67-decc-2fc93efaf60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(size_average=True, ignore_index=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_DoqJeh0sFq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "192ntOA0fSva",
        "colab_type": "code",
        "outputId": "fcda567a-0251-4d60-c521-336cf41a96eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "EPOCHS = 40\n",
        "teacher_forcing = 1\n",
        "import time\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    teacher_forcing = teacher_forcing * 0.95\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch, (english, hindi) in enumerate(dataloader):\n",
        "        loss = 0\n",
        "        if(english.size(0) != 64):\n",
        "          continue\n",
        "        else:\n",
        "          hidden = encoder.initHidden(english.size(0))\n",
        "\n",
        "          enc_output, enc_hidden = encoder(english.to(device), hidden.to(device))\n",
        "\n",
        "          dec_hidden = enc_hidden.view(2, english.size(0), -1)\n",
        "          dec_input = torch.tensor([[hindi_vocab_to_int['<start>']]] * english.size(0))\n",
        "          use_teacher_forcing = True if random.random() < teacher_forcing else False\n",
        "          if(use_teacher_forcing):\n",
        "            # use teacher forcing - feeding the target as the next input (via dec_input)\n",
        "            for t in range(1, hindi.size(1)):\n",
        "                predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                            dec_hidden.to(device), enc_output.to(device))\n",
        "                loss += criterion(predictions.to(device), hindi[:, t].to(device))\n",
        "                #loss += loss_\n",
        "                dec_input = hindi[:, t].unsqueeze(1)\n",
        "          else:\n",
        "            for t in range(1, hindi.size(1)):\n",
        "                predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                            dec_hidden.to(device), \n",
        "                                            enc_output.to(device))\n",
        "                loss += criterion(predictions.to(device), hindi[:, t].to(device))\n",
        "                topv, topi = predictions.topk(1)\n",
        "                dec_input = topi.detach()\n",
        "              \n",
        "          \n",
        "          batch_loss = loss\n",
        "          total_loss += batch_loss\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          loss.backward()\n",
        "\n",
        "          ### UPDATE MODEL PARAMETERS\n",
        "          optimizer.step()\n",
        "          \n",
        "          if batch % 100 == 0:\n",
        "              print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                          batch,\n",
        "                                                          batch_loss.detach().item()))\n",
        "          \n",
        "        \n",
        "    ### TODO: Save checkpoint for model\n",
        "    if epoch % 10 == 0 :\n",
        "      torch.save(encoder.state_dict(), '/content/encoder_3.pth')\n",
        "      torch.save(decoder.state_dict(), '/content/decoder_3.pth')\n",
        "\n",
        "    N_BATCH = english_train_text.shape[0] // 64\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 184.6284\n",
            "Epoch 1 Batch 100 Loss 111.7486\n",
            "Epoch 1 Batch 200 Loss 115.7352\n",
            "Epoch 1 Batch 300 Loss 101.7247\n",
            "Epoch 1 Batch 400 Loss 104.4151\n",
            "Epoch 1 Loss 108.9795\n",
            "Time taken for 1 epoch 46.44877481460571 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 103.3999\n",
            "Epoch 2 Batch 100 Loss 94.3581\n",
            "Epoch 2 Batch 200 Loss 99.3463\n",
            "Epoch 2 Batch 300 Loss 86.0952\n",
            "Epoch 2 Batch 400 Loss 100.7320\n",
            "Epoch 2 Loss 96.0890\n",
            "Time taken for 1 epoch 46.45458173751831 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 88.1574\n",
            "Epoch 3 Batch 100 Loss 90.2113\n",
            "Epoch 3 Batch 200 Loss 88.7948\n",
            "Epoch 3 Batch 300 Loss 89.9899\n",
            "Epoch 3 Batch 400 Loss 80.9804\n",
            "Epoch 3 Loss 88.2731\n",
            "Time taken for 1 epoch 46.35342073440552 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 77.9005\n",
            "Epoch 4 Batch 100 Loss 77.0469\n",
            "Epoch 4 Batch 200 Loss 76.6298\n",
            "Epoch 4 Batch 300 Loss 78.3594\n",
            "Epoch 4 Batch 400 Loss 75.5130\n",
            "Epoch 4 Loss 81.2206\n",
            "Time taken for 1 epoch 46.31776762008667 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 96.8357\n",
            "Epoch 5 Batch 100 Loss 71.5548\n",
            "Epoch 5 Batch 200 Loss 95.9240\n",
            "Epoch 5 Batch 300 Loss 68.4768\n",
            "Epoch 5 Batch 400 Loss 97.6896\n",
            "Epoch 5 Loss 76.1917\n",
            "Time taken for 1 epoch 46.35928273200989 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 93.6929\n",
            "Epoch 6 Batch 100 Loss 63.2201\n",
            "Epoch 6 Batch 200 Loss 59.6950\n",
            "Epoch 6 Batch 300 Loss 92.6643\n",
            "Epoch 6 Batch 400 Loss 62.6316\n",
            "Epoch 6 Loss 71.3080\n",
            "Time taken for 1 epoch 46.42075777053833 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 86.3012\n",
            "Epoch 7 Batch 100 Loss 57.0171\n",
            "Epoch 7 Batch 200 Loss 85.1842\n",
            "Epoch 7 Batch 300 Loss 56.1903\n",
            "Epoch 7 Batch 400 Loss 59.4787\n",
            "Epoch 7 Loss 66.1431\n",
            "Time taken for 1 epoch 46.30592918395996 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 51.7617\n",
            "Epoch 8 Batch 100 Loss 78.2970\n",
            "Epoch 8 Batch 200 Loss 84.1499\n",
            "Epoch 8 Batch 300 Loss 49.7120\n",
            "Epoch 8 Batch 400 Loss 51.2522\n",
            "Epoch 8 Loss 63.2615\n",
            "Time taken for 1 epoch 46.382360219955444 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 76.8860\n",
            "Epoch 9 Batch 100 Loss 47.8536\n",
            "Epoch 9 Batch 200 Loss 51.4489\n",
            "Epoch 9 Batch 300 Loss 80.5980\n",
            "Epoch 9 Batch 400 Loss 49.2405\n",
            "Epoch 9 Loss 59.9639\n",
            "Time taken for 1 epoch 46.44950604438782 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 72.6510\n",
            "Epoch 10 Batch 100 Loss 40.5382\n",
            "Epoch 10 Batch 200 Loss 40.2939\n",
            "Epoch 10 Batch 300 Loss 44.3320\n",
            "Epoch 10 Batch 400 Loss 41.5342\n",
            "Epoch 10 Loss 57.2085\n",
            "Time taken for 1 epoch 46.33977794647217 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 68.8328\n",
            "Epoch 11 Batch 100 Loss 42.1801\n",
            "Epoch 11 Batch 200 Loss 41.9353\n",
            "Epoch 11 Batch 300 Loss 72.9735\n",
            "Epoch 11 Batch 400 Loss 44.5394\n",
            "Epoch 11 Loss 54.8088\n",
            "Time taken for 1 epoch 46.62550187110901 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 72.0608\n",
            "Epoch 12 Batch 100 Loss 37.4738\n",
            "Epoch 12 Batch 200 Loss 71.7391\n",
            "Epoch 12 Batch 300 Loss 40.3474\n",
            "Epoch 12 Batch 400 Loss 66.7411\n",
            "Epoch 12 Loss 52.1843\n",
            "Time taken for 1 epoch 46.44418501853943 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 63.0171\n",
            "Epoch 13 Batch 100 Loss 63.2143\n",
            "Epoch 13 Batch 200 Loss 37.0737\n",
            "Epoch 13 Batch 300 Loss 70.3248\n",
            "Epoch 13 Batch 400 Loss 64.6972\n",
            "Epoch 13 Loss 51.1147\n",
            "Time taken for 1 epoch 46.429269552230835 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 30.8405\n",
            "Epoch 14 Batch 100 Loss 34.5635\n",
            "Epoch 14 Batch 200 Loss 60.6829\n",
            "Epoch 14 Batch 300 Loss 37.7968\n",
            "Epoch 14 Batch 400 Loss 65.2504\n",
            "Epoch 14 Loss 49.2597\n",
            "Time taken for 1 epoch 46.476125955581665 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 28.7491\n",
            "Epoch 15 Batch 100 Loss 61.5573\n",
            "Epoch 15 Batch 200 Loss 31.4208\n",
            "Epoch 15 Batch 300 Loss 35.5944\n",
            "Epoch 15 Batch 400 Loss 61.5428\n",
            "Epoch 15 Loss 46.3297\n",
            "Time taken for 1 epoch 46.38994836807251 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 27.7133\n",
            "Epoch 16 Batch 100 Loss 53.7918\n",
            "Epoch 16 Batch 200 Loss 57.3071\n",
            "Epoch 16 Batch 300 Loss 29.9937\n",
            "Epoch 16 Batch 400 Loss 34.4378\n",
            "Epoch 16 Loss 46.4000\n",
            "Time taken for 1 epoch 46.44609761238098 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 51.4720\n",
            "Epoch 17 Batch 100 Loss 28.5592\n",
            "Epoch 17 Batch 200 Loss 50.0524\n",
            "Epoch 17 Batch 300 Loss 54.0315\n",
            "Epoch 17 Batch 400 Loss 29.8380\n",
            "Epoch 17 Loss 43.9210\n",
            "Time taken for 1 epoch 46.40774416923523 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 25.7815\n",
            "Epoch 18 Batch 100 Loss 49.3178\n",
            "Epoch 18 Batch 200 Loss 53.7753\n",
            "Epoch 18 Batch 300 Loss 54.0572\n",
            "Epoch 18 Batch 400 Loss 27.8777\n",
            "Epoch 18 Loss 42.0717\n",
            "Time taken for 1 epoch 46.45860695838928 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 47.4169\n",
            "Epoch 19 Batch 100 Loss 52.2149\n",
            "Epoch 19 Batch 200 Loss 50.1600\n",
            "Epoch 19 Batch 300 Loss 49.0952\n",
            "Epoch 19 Batch 400 Loss 49.2555\n",
            "Epoch 19 Loss 40.3189\n",
            "Time taken for 1 epoch 46.42150402069092 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 43.5966\n",
            "Epoch 20 Batch 100 Loss 42.8855\n",
            "Epoch 20 Batch 200 Loss 44.7467\n",
            "Epoch 20 Batch 300 Loss 51.7477\n",
            "Epoch 20 Batch 400 Loss 46.8453\n",
            "Epoch 20 Loss 39.9144\n",
            "Time taken for 1 epoch 46.44666600227356 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 41.9371\n",
            "Epoch 21 Batch 100 Loss 47.8126\n",
            "Epoch 21 Batch 200 Loss 25.9071\n",
            "Epoch 21 Batch 300 Loss 29.2446\n",
            "Epoch 21 Batch 400 Loss 49.9851\n",
            "Epoch 21 Loss 38.6946\n",
            "Time taken for 1 epoch 46.594828605651855 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 44.0636\n",
            "Epoch 22 Batch 100 Loss 26.3409\n",
            "Epoch 22 Batch 200 Loss 31.5534\n",
            "Epoch 22 Batch 300 Loss 40.1391\n",
            "Epoch 22 Batch 400 Loss 44.9911\n",
            "Epoch 22 Loss 37.0384\n",
            "Time taken for 1 epoch 46.45320415496826 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 37.2731\n",
            "Epoch 23 Batch 100 Loss 37.4386\n",
            "Epoch 23 Batch 200 Loss 39.9191\n",
            "Epoch 23 Batch 300 Loss 21.2517\n",
            "Epoch 23 Batch 400 Loss 40.7035\n",
            "Epoch 23 Loss 36.0474\n",
            "Time taken for 1 epoch 46.55833411216736 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 20.9174\n",
            "Epoch 24 Batch 100 Loss 38.3931\n",
            "Epoch 24 Batch 200 Loss 41.5469\n",
            "Epoch 24 Batch 300 Loss 23.0255\n",
            "Epoch 24 Batch 400 Loss 41.8589\n",
            "Epoch 24 Loss 33.7878\n",
            "Time taken for 1 epoch 46.44046211242676 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 31.6276\n",
            "Epoch 25 Batch 100 Loss 20.4186\n",
            "Epoch 25 Batch 200 Loss 37.8903\n",
            "Epoch 25 Batch 300 Loss 37.0006\n",
            "Epoch 25 Batch 400 Loss 38.4312\n",
            "Epoch 25 Loss 32.8967\n",
            "Time taken for 1 epoch 46.47855997085571 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 30.3531\n",
            "Epoch 26 Batch 100 Loss 38.7601\n",
            "Epoch 26 Batch 200 Loss 32.7811\n",
            "Epoch 26 Batch 300 Loss 36.2852\n",
            "Epoch 26 Batch 400 Loss 20.3350\n",
            "Epoch 26 Loss 32.2605\n",
            "Time taken for 1 epoch 46.52434158325195 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 17.2285\n",
            "Epoch 27 Batch 100 Loss 39.2141\n",
            "Epoch 27 Batch 200 Loss 30.5607\n",
            "Epoch 27 Batch 300 Loss 22.8882\n",
            "Epoch 27 Batch 400 Loss 32.3018\n",
            "Epoch 27 Loss 30.2840\n",
            "Time taken for 1 epoch 46.55415892601013 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 19.0516\n",
            "Epoch 28 Batch 100 Loss 29.5596\n",
            "Epoch 28 Batch 200 Loss 34.1006\n",
            "Epoch 28 Batch 300 Loss 20.8469\n",
            "Epoch 28 Batch 400 Loss 32.4067\n",
            "Epoch 28 Loss 30.3444\n",
            "Time taken for 1 epoch 46.45577573776245 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 33.9403\n",
            "Epoch 29 Batch 100 Loss 37.7391\n",
            "Epoch 29 Batch 200 Loss 18.4524\n",
            "Epoch 29 Batch 300 Loss 43.7886\n",
            "Epoch 29 Batch 400 Loss 38.2019\n",
            "Epoch 29 Loss 29.6170\n",
            "Time taken for 1 epoch 46.39272379875183 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 15.8837\n",
            "Epoch 30 Batch 100 Loss 17.9888\n",
            "Epoch 30 Batch 200 Loss 29.6379\n",
            "Epoch 30 Batch 300 Loss 31.1364\n",
            "Epoch 30 Batch 400 Loss 31.8367\n",
            "Epoch 30 Loss 28.5782\n",
            "Time taken for 1 epoch 46.4072105884552 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 29.6010\n",
            "Epoch 31 Batch 100 Loss 27.2836\n",
            "Epoch 31 Batch 200 Loss 27.5132\n",
            "Epoch 31 Batch 300 Loss 18.6854\n",
            "Epoch 31 Batch 400 Loss 30.8402\n",
            "Epoch 31 Loss 27.0052\n",
            "Time taken for 1 epoch 46.589033126831055 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 25.8982\n",
            "Epoch 32 Batch 100 Loss 28.6654\n",
            "Epoch 32 Batch 200 Loss 14.4928\n",
            "Epoch 32 Batch 300 Loss 26.8265\n",
            "Epoch 32 Batch 400 Loss 34.1784\n",
            "Epoch 32 Loss 26.3213\n",
            "Time taken for 1 epoch 46.493571281433105 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 28.5770\n",
            "Epoch 33 Batch 100 Loss 15.4827\n",
            "Epoch 33 Batch 200 Loss 28.4857\n",
            "Epoch 33 Batch 300 Loss 30.3460\n",
            "Epoch 33 Batch 400 Loss 30.5790\n",
            "Epoch 33 Loss 25.9119\n",
            "Time taken for 1 epoch 46.45721650123596 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 25.1890\n",
            "Epoch 34 Batch 100 Loss 30.5931\n",
            "Epoch 34 Batch 200 Loss 25.9990\n",
            "Epoch 34 Batch 300 Loss 26.8336\n",
            "Epoch 34 Batch 400 Loss 25.3248\n",
            "Epoch 34 Loss 24.9721\n",
            "Time taken for 1 epoch 46.4144766330719 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 22.5876\n",
            "Epoch 35 Batch 100 Loss 28.0665\n",
            "Epoch 35 Batch 200 Loss 14.0947\n",
            "Epoch 35 Batch 300 Loss 16.5130\n",
            "Epoch 35 Batch 400 Loss 18.2419\n",
            "Epoch 35 Loss 24.0557\n",
            "Time taken for 1 epoch 46.467376708984375 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 23.3302\n",
            "Epoch 36 Batch 100 Loss 17.0892\n",
            "Epoch 36 Batch 200 Loss 22.1037\n",
            "Epoch 36 Batch 300 Loss 24.7396\n",
            "Epoch 36 Batch 400 Loss 24.8305\n",
            "Epoch 36 Loss 23.2405\n",
            "Time taken for 1 epoch 46.5250039100647 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 14.3584\n",
            "Epoch 37 Batch 100 Loss 23.4845\n",
            "Epoch 37 Batch 200 Loss 22.9717\n",
            "Epoch 37 Batch 300 Loss 23.0670\n",
            "Epoch 37 Batch 400 Loss 26.4724\n",
            "Epoch 37 Loss 22.8001\n",
            "Time taken for 1 epoch 46.49608278274536 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 24.3064\n",
            "Epoch 38 Batch 100 Loss 28.4995\n",
            "Epoch 38 Batch 200 Loss 22.0566\n",
            "Epoch 38 Batch 300 Loss 24.5820\n",
            "Epoch 38 Batch 400 Loss 21.0993\n",
            "Epoch 38 Loss 21.8369\n",
            "Time taken for 1 epoch 46.4417986869812 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 15.7946\n",
            "Epoch 39 Batch 100 Loss 20.5920\n",
            "Epoch 39 Batch 200 Loss 30.1295\n",
            "Epoch 39 Batch 300 Loss 20.1029\n",
            "Epoch 39 Batch 400 Loss 23.9270\n",
            "Epoch 39 Loss 21.4842\n",
            "Time taken for 1 epoch 46.614829540252686 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 11.4240\n",
            "Epoch 40 Batch 100 Loss 13.8214\n",
            "Epoch 40 Batch 200 Loss 19.3597\n",
            "Epoch 40 Batch 300 Loss 20.4968\n",
            "Epoch 40 Batch 400 Loss 14.4148\n",
            "Epoch 40 Loss 20.5651\n",
            "Time taken for 1 epoch 46.50745129585266 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtdnARFOHgo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(encoder.state_dict(), '/content/encoder_3.pth')\n",
        "torch.save(decoder.state_dict(), '/content/decoder_3.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJU29FRhwoen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(text, encoder, decoder):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    hidden = encoder.initHidden(1)\n",
        "    input_text = [english_vocab_to_int[x.lower()] for x in text.split()]\n",
        "    input_text = torch.from_numpy(np.array(input_text)).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "      enc_output, enc_hidden = encoder(input_text.to(device), hidden.to(device))\n",
        "      dec_hidden = enc_hidden.view(2, 1, -1)\n",
        "      dec_input = torch.tensor([[hindi_vocab_to_int['<start>']]])\n",
        "      decoder_attentions = torch.zeros(9, 9)\n",
        "      decoded_words = []\n",
        "      for di in range(19):\n",
        "         predictions, dec_hidden, _ = decoder(dec_input.to(device), dec_hidden.to(device), \n",
        "                                              enc_output.to(device))\n",
        "         topv, topi = predictions.data.topk(1)\n",
        "         if topi.item() == hindi_vocab_to_int['<end>']:\n",
        "          decoded_words.append('<end>')\n",
        "          break\n",
        "         else:\n",
        "          decoded_words.append(hindi_int_to_vocab[topi.item()])\n",
        "          dec_input = topi\n",
        "    return(decoded_words)\n",
        "         \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHj2uYlyL76-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test1 = [''.join(english_int_to_vocab[x]) for x in list(english_test_text[0]) if x != 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNq5UUoWOJrN",
        "colab_type": "code",
        "outputId": "db5638af-046d-4d8e-c7a2-bcb59a636b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "text = 'uttra was married to abhimanyu <end>'\n",
        "inference(text, encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['दासबाबू',\n",
              " 'दासबाबू',\n",
              " 'में',\n",
              " 'नरेंद्र',\n",
              " 'भाई',\n",
              " 'से',\n",
              " 'कुलीन',\n",
              " 'देवी',\n",
              " 'देवी',\n",
              " 'जो',\n",
              " 'से',\n",
              " 'से',\n",
              " 'हुआ',\n",
              " 'था।',\n",
              " '<end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPYNPcXvlTEQ",
        "colab_type": "code",
        "outputId": "93b102df-c4f2-472d-8fb7-b07c0603cd18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "max(english_int_to_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9942"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNh8H2DYrV7a",
        "colab_type": "code",
        "outputId": "f81faec0-1b37-4e0e-ac9e-7ff617226453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset.source.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ted', 'indic2012', 'tides'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    }
  ]
}